{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karank85/speech-recognition/blob/main/Project2_DL_Speech_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZycw3QagLpa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "import librosa\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import librosa.display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "\n",
        "import glob\n",
        "\n",
        "!pip install transformers datasets evaluate jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "librosa.__version__"
      ],
      "metadata": {
        "id": "3w9t1VF0gRQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "F_jS9kfO3wpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hsP0jZs-gTMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumptions:\n",
        "# - The transcription file is located in the same directory as the audio files.\n",
        "class AudioDataset:\n",
        "  \"\"\"\n",
        "  Class for loading and storing audio data.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.df = pd.DataFrame(columns=['id', 'path', 'transcription'])\n",
        "\n",
        "  def load_transcriptions(self, directory_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Load all transcriptions from a given directory, including subdirectories.\n",
        "    Returns False if no transcription files were found, or if any failed to load.\n",
        "    \"\"\"\n",
        "    transcriptions_path = glob.glob(\n",
        "        f\"{directory_path}/**/*.trans.txt\",\n",
        "        recursive=True\n",
        "    )\n",
        "\n",
        "    if len(transcriptions_path) == 0:\n",
        "      return False\n",
        "\n",
        "    for path in transcriptions_path:\n",
        "      if not self.load_transcription_file(path):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "  def load_transcription_file(self, file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Parse transcription file and records the audio ID - subtitle mapping.\n",
        "    Returns False if the file could not be read.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\") as file:\n",
        "      file_directory = os.path.dirname(file_path)\n",
        "\n",
        "      lines = file.read().split(\"\\n\")\n",
        "      for line in lines:\n",
        "        if len(line.strip()) == 0:\n",
        "          continue\n",
        "        splitter = line.split(\" \")\n",
        "        file_name = splitter[0]\n",
        "        file_content = ' '.join(splitter[1:])\n",
        "        self.df.loc[len(self.df)] = {\n",
        "            'id':file_name,\n",
        "            'transcription':file_content,\n",
        "            'path': f'{file_directory}/{file_name}.flac'\n",
        "        }\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def keys(self):\n",
        "    return iter(self.df['id'])\n",
        "\n",
        "  def get(self, id: int):\n",
        "    \"\"\"\n",
        "    Retrieve a dataframe row from ID.\n",
        "    \"\"\"\n",
        "    return self.df.loc[self.df['id'] == id]"
      ],
      "metadata": {
        "id": "rQKj6v9RgdKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = AudioDataset()"
      ],
      "metadata": {
        "id": "uFti1k8ighEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.load_transcriptions(\"/content/drive/MyDrive/\")"
      ],
      "metadata": {
        "id": "FC6BlmBgghWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_list = []\n",
        "sampling_freq_list = []\n",
        "\n",
        "for index, row in ds.df.iterrows():\n",
        "  audio, sampling_freq = librosa.load(row['path'], sr=16_000)\n",
        "\n",
        "  audio_list.append(audio)\n",
        "  sampling_freq_list.append(sampling_freq)\n",
        "\n",
        "ds.df['audio'] = audio_list\n",
        "ds.df['sampling_freq'] = sampling_freq_list"
      ],
      "metadata": {
        "id": "CUB0YyBgyQzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Features, Array3D, Value\n",
        "\n",
        "custom_dataset = Dataset.from_pandas(ds.df)\n",
        "custom_dataset"
      ],
      "metadata": {
        "id": "-X_R8KuSoIA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset = custom_dataset.remove_columns(['path', 'id', '__index_level_0__'])\n",
        "# transcription: str, audio: list[int], sampling_freq: int\n",
        "custom_dataset"
      ],
      "metadata": {
        "id": "-t7Nuh2x0tvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making sure it's all uppercase characters as Wav2Vec Tokenizer is only trained\n",
        "# on uppercase characters and we need to match the tokenizer's vocabulary.\n",
        "for item in custom_dataset[\"train\"]:\n",
        "  print(item[\"transcription\"])"
      ],
      "metadata": {
        "id": "qZcS35Sy-8RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")"
      ],
      "metadata": {
        "id": "0u7TJisQoczk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset = custom_dataset.train_test_split(test_size=0.2)\n",
        "custom_dataset"
      ],
      "metadata": {
        "id": "6Z0a-oeX7BRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_processor(batch):\n",
        "  batch = processor(batch[\"audio\"], sampling_rate=batch[\"sampling_freq\"], text=batch[\"transcription\"])\n",
        "  batch[\"input_length\"] = len(batch[\"input_values\"][0])\n",
        "  return batch\n",
        "\n",
        "encoded_datasets = custom_dataset.map(apply_processor, remove_columns=custom_dataset.column_names[\"train\"], num_proc=4)"
      ],
      "metadata": {
        "id": "CB97Kll01geD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to create a data collator to prepare batches of data suitable for training CTC loss-based models\n",
        "# Pad text and labels to length of the longest element in its batch to make it uniform length\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCLossWithPadding:\n",
        "  processor: AutoProcessor\n",
        "  padding: Union[bool, str] = \"longest\" # pad to the longest sequence\n",
        "\n",
        "  def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "    #audio features\n",
        "    input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n",
        "    # tokenized labels\n",
        "    label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "    batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
        "\n",
        "    labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n",
        "\n",
        "    # replace padding with -100 to ignore loss correctly\n",
        "    labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attentions_mask.ne(1), -100)\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "n7fMhqGrAX-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorCTCLossWithPadding(processor=processor, padding=\"longest\")"
      ],
      "metadata": {
        "id": "IWgnruMcIqms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wave2Vec2.0"
      ],
      "metadata": {
        "id": "TMiiBtWW4tii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\")"
      ],
      "metadata": {
        "id": "rx6yOxj4lMre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "wer = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "Qra36dYO1_Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    # Compute predicted labels\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "\n",
        "    # Replace -100 with pad token ID in true labels\n",
        "    true_labels = np.where(pred.label_ids == -100, processor.tokenizer.pad_token_id, pred.label_ids)\n",
        "    true_str = processor.batch_decode(true_labels, group_tokens=False)\n",
        "\n",
        "    # Compute Word Error Rate (WER)\n",
        "    wer_score = wer.compute(predictions=pred_str, references=true_str)\n",
        "\n",
        "    return {\"wer\": wer_score}\n"
      ],
      "metadata": {
        "id": "6o9k0lfV3yMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCTC\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "qzBrYrqAC6IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"wav2vec_model_v1\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=2000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    group_by_length=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_datasets[\"train\"],\n",
        "    eval_dataset=encoded_datasets[\"test\"],\n",
        "    tokenizer=processor,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "omPJvI78DFEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3gsgUzcDGj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}